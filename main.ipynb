{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from networkx.algorithms import bipartite\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fossil Fuel Subsidies"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we need to keep values titled \"NA\" because that's Namibia's ISO2 code. Also, Kosovo doesn't have an ISO3 so we can't use that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsidy = pd.read_csv('input/policy/Fossil_Fuel_Subsidies.csv', index_col=0, keep_default_na=False, na_values=\"\")\n",
    "subsidy.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_nodes = subsidy.ISO2.unique()\n",
    "policy_nodes = subsidy.CTS_Code.unique()\n",
    "print(\"Number of Countries:\", len(country_nodes))\n",
    "print(\"Number of Subsidies:\", len(policy_nodes))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another question would be how do we create groups of subsidies. We'll call the fossil fuel subsidy bi-partite graph S."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = nx.Graph()\n",
    "S.add_nodes_from(country_nodes, bipartite=0)\n",
    "S.add_nodes_from(policy_nodes, bipartite=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the following countries do not have the Petroleum field. We'll simply not draw edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full = set(subsidy[subsidy.ISO2 == \"AF\"].CTS_Name)\n",
    "subsidy_sub = subsidy[subsidy.Unit == \"Percent of GDP\"]\n",
    "for country in country_nodes:\n",
    "    subset = subsidy_sub[subsidy_sub.ISO2 == country]\n",
    "    if len(subset) < 21:\n",
    "        print(country, len(subset))\n",
    "        partial = set(subsidy[subsidy.ISO2 == country].CTS_Name)\n",
    "        print(full - partial)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop through every country/policy combination and add an edge if the value for specified year was greater than 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of Edges:\", S.number_of_edges())\n",
    "print(\"Number of Possible Edges:\", len(country_nodes) * len(policy_nodes))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change Point Detection\n",
    "\n",
    "NEED TO FINISH"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering\n",
    "\n",
    "NEED TO FINSIH"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Green Bonds"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "green_bonds = pd.read_csv(\"input/policy/Green_Bonds.csv\", index_col=0, keep_default_na=False, na_values=\"\")\n",
    "\n",
    "# Filtered Down to countries\n",
    "green_bonds = green_bonds[np.logical_not(green_bonds[\"ISO2\"].isna())]\n",
    "\n",
    "green_bond_country_nodes = green_bonds.ISO2.unique()\n",
    "# Need to figure out the differences between these bonds\n",
    "bonds_nodes = green_bonds[\"CTS_Name\"].unique()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks likes most years no one invested in bonds. Still very few countries invest in bonds but each year it seems the number increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the data\n",
    "num_countries = green_bonds[green_bonds.columns[12:]].count().to_list()\n",
    "years = [int(year[1:]) for year in green_bonds.columns[12:]]\n",
    "\n",
    "# Plot\n",
    "fig,ax = plt.subplots()\n",
    "ax.scatter(years, num_countries)\n",
    "ax.set_title(\"Number of Countries using Green Bonds by year\")\n",
    "ax.set_ylabel(\"Number of Countries\")\n",
    "ax.set_xlabel(\"Year\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore we can break the graph down into its two different categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the data\n",
    "issuances = green_bonds[green_bonds[\"CTS_Name\"] == \"Green Bonds Issuances\"]\n",
    "\n",
    "\n",
    "# Calculate Variables\n",
    "issuance_count = issuances[issuances.columns[12:]].count()\n",
    "nonissuance_count = num_countries - issuance_count\n",
    "y = np.vstack([nonissuance_count, issuance_count])\n",
    "\n",
    "# Plot\n",
    "fig,ax = plt.subplots()\n",
    "# Using years var from last cell\n",
    "ax.stackplot(years, y, labels = [\"Nonissuance Green Bonds\", \"Issuance Green Bonds\",])\n",
    "ax.set_title(\"Number of Countries using Green Bonds by year\")\n",
    "ax.set_ylabel(\"Number of Countries\")\n",
    "ax.set_xlabel(\"Year\")\n",
    "ax.legend(loc = \"upper left\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the data\n",
    "issuances = green_bonds[green_bonds[\"CTS_Name\"] == \"Green Bonds Issuances\"]\n",
    "nonissuances = green_bonds[green_bonds[\"CTS_Name\"] == \"Green Bonds\"]\n",
    "\n",
    "# Calculate Variables\n",
    "issuance_money = issuances[issuances.columns[12:]].fillna(0).sum()\n",
    "nonissuance_money = nonissuances[nonissuances.columns[12:]].fillna(0).sum()\n",
    "y = np.vstack([nonissuance_money, issuance_money])\n",
    "\n",
    "# Plot\n",
    "fig,ax = plt.subplots()\n",
    "# Using years var from last cell\n",
    "ax.stackplot(years, y, labels = [\"Nonissuance Green Bonds\", \"Issuance Green Bonds\",])\n",
    "ax.set_title(\"Number of Countries using Green Bonds by year\")\n",
    "ax.set_ylabel(\"Number of Countries\")\n",
    "ax.set_xlabel(\"Year\")\n",
    "ax.legend(loc = \"upper left\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph Building"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing the nodes in our bipartite graph. Country nodes are on the left and green bond policies are on the right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = nx.Graph()\n",
    "B.add_nodes_from(green_bond_country_nodes, bipartite=0)\n",
    "B.add_nodes_from(bonds_nodes, bipartite=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are creating the edge_lists for every single year. This can allow for quick creation of graphs for any specific year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "green_bonds_temp = green_bonds.set_index([\"ISO2\", \"CTS_Name\"], inplace = False)\n",
    "green_bond_years = []\n",
    "green_bond_dict = {}\n",
    "\n",
    "for year in green_bonds.columns[12:]: #create edge lists for each year\n",
    "    edge_list = []\n",
    "    for country in green_bond_country_nodes: #find every country-bond combo\n",
    "        subset = green_bonds_temp.loc[country]\n",
    "        for bond in subset.index: #bond\n",
    "            weight = subset.loc[bond][year] #edge weights are bond values\n",
    "            if weight > 0:\n",
    "                edge = (country, bond, weight) #create edge\n",
    "                edge_list.append(edge)\n",
    "    # Take out years with no policies\n",
    "    if not (len(edge_list) == 0):\n",
    "        green_bond_years.append(int(year[1:]))\n",
    "        green_bond_dict[int(year[1:])] = edge_list\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a graph for 2015 and see what it looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B.remove_edges_from(B.edges)\n",
    "B.add_weighted_edges_from(green_bond_dict[2015])\n",
    "print(\"Number of Edges:\", B.number_of_edges())\n",
    "print(\"Number of Possible Edges:\", len(green_bond_country_nodes) * len(bonds_nodes))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also create a method to see the number of countries implementing any policies at all:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def country_count(B, country_nodes):\n",
    "    deg_policy, deg_country = bipartite.degrees(B, country_nodes)\n",
    "    sum = 0\n",
    "    for c, d in deg_country:\n",
    "        if d > 0:\n",
    "            sum += 1\n",
    "    return sum\n",
    "print(\"Participating Countries:\", country_count(B, green_bond_country_nodes))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change Point Detection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "windowSize = 6\n",
    "\n",
    "\n",
    "def change_point_detection(windowSize, G, years, edge_dict):\n",
    "\n",
    "    dict_signatures = {}\n",
    "    for year in years:\n",
    "        G.remove_edges_from(G.edges)\n",
    "        G.add_weighted_edges_from(edge_dict[year])\n",
    "\n",
    "\n",
    "        # Create the signless Laplacian but found the normalized laplacian to be better\n",
    "        # A = nx.adjacency_matrix(B)\n",
    "        # D = np.diag(nx.laplacian_matrix(B).diagonal())\n",
    "\n",
    "        laplacian = nx.normalized_laplacian_matrix(G).toarray()\n",
    "        singular_values = np.linalg.svd(laplacian)[1]\n",
    "        l2_norm = np.linalg.norm(singular_values, ord = 2)\n",
    "        norm_singluar_values = singular_values / l2_norm\n",
    "\n",
    "        dict_signatures[year] = norm_singluar_values\n",
    "\n",
    "    window_dicts = {}\n",
    "    for i in range(len(years) - windowSize):\n",
    "        year_subset = years[i:i+windowSize]\n",
    "\n",
    "        # Choose one random signature to start our weights_avg\n",
    "        weights_avg = np.zeros(dict_signatures[2018].shape)\n",
    "\n",
    "        for year in year_subset:\n",
    "            weights_avg += dict_signatures[year] * (1/windowSize)\n",
    "\n",
    "        window_dicts[years[windowSize + i]] = weights_avg / np.linalg.norm(weights_avg, ord = 2)\n",
    "\n",
    "\n",
    "    Z_list = []\n",
    "    year_list = []\n",
    "\n",
    "    for year in window_dicts.keys():\n",
    "        baseline = window_dicts[year]\n",
    "        single_year = dict_signatures[year]\n",
    "        Z = 1-np.transpose(single_year) @ baseline\n",
    "\n",
    "        Z_list.append(Z)\n",
    "        year_list.append(year)\n",
    "\n",
    "    return [Z_list, year_list]\n",
    "    \n",
    "Z_list, year_list = change_point_detection(windowSize, B, green_bond_years, green_bond_dict)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Successfully was able to show that the biggest change was in 2010s. Need to update method to take out laplacians that are 0, but I feel like this is important information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots()\n",
    "# Using years var from last cell\n",
    "ax.plot(year_list, Z_list)\n",
    "ax.set_title(\"Change Point Detection for Green Bonds \\n Window of 6 years\")\n",
    "ax.set_ylabel(\"Change Value\")\n",
    "ax.set_xlabel(\"Year\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Temporal graph creation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for year in green_bond_years:\n",
    "    B.remove_edges_from(B.edges)\n",
    "    B.add_weighted_edges_from(dict[year])\n",
    "    countries = country_count(B, green_bond_country_nodes) #participating countries\n",
    "    results.append({\"Year\": year, \"Countries\": countries, \"Edges\": B.number_of_edges()})\n",
    "pd.DataFrame(results)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things to implement temporally: (loop through every year, show how these change)\n",
    "- number of nodes (remove those without any edges)\n",
    "- number of edges (weighted)\n",
    "- number of trianges -> metamorphis coefficient\n",
    "- change point detection\n",
    "- any other correlation measures\n",
    "\n",
    "Node based metrics:\n",
    "- harmonic, closeness, degree, & eigenvector centralities\n",
    "- you can show how it changes over time\n",
    "\n",
    "Clustering:\n",
    "- find out how to cluster bipartite graphs\n",
    "- change point detection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code maps each policy and country to its corresponding numerical id that will be used in the BINE code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "green_bond_node_mapping = {}\n",
    "\n",
    "# Mapping for contries\n",
    "i = 0\n",
    "for country in green_bonds[\"ISO2\"].unique():\n",
    "    green_bond_node_mapping[country] = f\"u{i}\"\n",
    "    i += 1\n",
    "\n",
    "# Mapping for policies\n",
    "j = 0\n",
    "for policy in green_bonds[\"CTS_Name\"].unique():\n",
    "    green_bond_node_mapping[policy] = f\"i{j}\"\n",
    "    j += 1\n",
    "\n",
    "# Reverses the mapping to be id to name\n",
    "inv_green_bond_mapping = {v: k for k, v in green_bond_node_mapping.items()}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 2 parts to the next code block:\n",
    "\n",
    "1. First, we create edgelists for each graph in a format the BINE algorithm can read\n",
    "1. Second we read each edgelist and create a command to run in the terminal that obtains the BINE embeddings\n",
    "\n",
    "Note: The reason we have to write these commands to a new text file is because we need to run an old version of python for BINE algorithm to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Create graphs BINE can read\n",
    "for year in green_bond_years:\n",
    "    B.remove_edges_from(B.edges)\n",
    "    B.add_weighted_edges_from(green_bond_dict[year])\n",
    "\n",
    "    # Relabels our graph from name to id to work on BINE code\n",
    "    B_integer = nx.relabel_nodes(B, green_bond_node_mapping)\n",
    "\n",
    "    fileName = f\"BiNE-master/data/greenbonds/greenbonds_{year}_edgelist.dat\"\n",
    "    nx.write_weighted_edgelist(B_integer, fileName, delimiter = \"\\t\")\n",
    "\n",
    "# Create commands to run BINE file.\n",
    "with open(\"Green_bond_commands.txt\", \"w\") as f:\n",
    "    for year in green_bond_years:\n",
    "        command = f\"python train.py --train-data ../data/greenbonds/greenbonds_{year}_edgelist.dat --lam 0.025 --d 10 --max-iter 100 --large 2 --vectors-u ../data/greenbonds/vectors_u_{year}.dat --vectors-v ../data/greenbonds/vectors_v_{year}.dat &&\"\n",
    "        f.write(command)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have feature representations of our countries we can complete a cluster analysis with them. \n",
    "\n",
    "Between the code block above and the code block below, we must run the commands file to obtain all the BINE embeddings. After we obtain the BINE embeddings, we then can read them into a pandas file for simple analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bine_embeddings_bonds_dict = {}\n",
    "\n",
    "for year in green_bond_years:\n",
    "\n",
    "    out_mat = []\n",
    "    cols = []\n",
    "    countries = []\n",
    "\n",
    "    # This code is only opening the BINE embeddings for a specific year.\n",
    "    with open(f\"BiNE-master/data/greenbonds/vectors_u_{year}.dat\", \"r\") as f:\n",
    "        for line in f:\n",
    "            data = line.split()\n",
    "\n",
    "            # Parses what country our embedding is of.\n",
    "            countries.append(data[0])\n",
    "\n",
    "            # Obtains the embedding for country\n",
    "            values = [float(elt) for elt in data[1:]]\n",
    "\n",
    "            # Reshapes the embedding\n",
    "            col = np.array(values).reshape((len(values),1))\n",
    "            cols.append(col)\n",
    "\n",
    "        out_mat = np.concatenate(cols, axis =1)\n",
    "\n",
    "    # Changes ids of countries to their actual country names\n",
    "    country_names = [inv_green_bond_mapping[country] for country in countries]\n",
    "\n",
    "    pandas_bine_embeddings_bonds = pd.DataFrame(out_mat, columns = country_names)\n",
    "\n",
    "    bine_embeddings_bonds_dict[year] = pandas_bine_embeddings_bonds"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets view what each embedding looks like for each country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_bine_embeddings_bonds = bine_embeddings_bonds_dict[2021]\n",
    "pandas_bine_embeddings_bonds"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we officially cluster, I will use the Shilloute Score to see how many cluster we should use in our K-Means clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# First we obtain the clusterings on the BINE embeddings using K\n",
    "\n",
    "X = pandas_bine_embeddings_bonds.transpose()\n",
    "\n",
    "silscore_x = []\n",
    "silscore_y = []\n",
    "\n",
    "for k in range(2,40):\n",
    "    clusterer = KMeans(n_clusters = k, random_state=10, n_init=10)\n",
    "    clustering= clusterer.fit(X)\n",
    "    score = silhouette_score(X, clustering.labels_)\n",
    "\n",
    "    silscore_x.append(k)\n",
    "    silscore_y.append(score)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "scatter = ax.plot(silscore_x, silscore_y)\n",
    "ax.set_title(\"Shilhouette Score Plot Green Bonds\")\n",
    "ax.set_ylabel(\"Shilhouette Score\")\n",
    "ax.set_xlabel(\"Number of Clusters\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above shilhouette scores it seems that the best number of clusters is 22, which means there will be about 3 countries per cluster. The clusters will then be pretty small. This also might be because the greenbonds is a pretty sparse dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have examined a good clustering of the data, we can visualize our clutering. First we will try the optimal 22 clusters and then try a simplier one like 3 clusters."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a baseline we can also look at the clustering if we just look at the column of each adjacency matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = []\n",
    "B.remove_edges_from(B.edges)\n",
    "B.add_weighted_edges_from(green_bond_dict[2021])\n",
    "countries_in_green_bonds = []\n",
    "\n",
    "for node in B.nodes(data=True):\n",
    "\n",
    "    # obtains nodes in partition 0, which is the country nodes\n",
    "    if node[1][\"bipartite\"] == 0:\n",
    "\n",
    "        col = np.zeros((2,1))\n",
    "        # Obtains list of all edges that country is in\n",
    "        for connect in B.edges(node[0], data = True):\n",
    "            # Gets weight for Green Bonds Issuances\n",
    "            if connect[1] == \"Green Bonds Issuances\":\n",
    "                col[0,0] = connect[2][\"weight\"]\n",
    "            # Gets weight for NonGreen Bonds Issuances\n",
    "            else:\n",
    "                col[1,0] = connect[2][\"weight\"]\n",
    "\n",
    "        if not (col.sum() == 0):\n",
    "            countries_in_green_bonds.append(node[0])\n",
    "            columns.append(col)\n",
    "\n",
    "sorted_indicies = np.argsort(np.array(countries_in_green_bonds))\n",
    "graph_data = np.concatenate(columns, axis = 1)[:,sorted_indicies]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we can make the clustering for the year 2021 using the normal clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_b = KMeans(n_clusters = 3, n_init= 10).fit(graph_data.transpose())\n",
    "# clustering.labels_\n",
    "\n",
    "fig,ax = plt.subplots()\n",
    "# Using years var from last cell\n",
    "scatter = ax.scatter(graph_data[0, :], graph_data[1,:], c = clustering_b.labels_)\n",
    "ax.set_title(\"Countries Clustering in Greenbond Bond Investments in 2021 \")\n",
    "ax.set_ylabel(\"Nonissuance Green Bonds\")\n",
    "ax.set_xlabel(\"Issuance Green Bonds\")\n",
    "legend1 = ax.legend(*scatter.legend_elements(),\n",
    "                    loc=\"upper left\", title=\"Classes\")\n",
    "ax.add_artist(legend1)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is helper code to get a mapping between the country name abbreviations and the full country name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0 \n",
    "country_abv_to_full = {}\n",
    "    \n",
    "country_ab = green_bonds[\"ISO2\"].unique()\n",
    "country_list = green_bonds[\"Country\"].unique()\n",
    "\n",
    "for i in range(len(country_ab)):\n",
    "    country_abv_to_full[country_ab[i]] = country_list[i]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code takes the labels of the countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for abv in np.array(country_names)[clustering.labels_[np.argsort(np.array(country_names))] == 38]:\n",
    "    print(country_abv_to_full[abv])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bine_labels = np.array(clustering.labels_)[np.argsort(np.array(country_names))]\n",
    "sorted_countries = np.array(country_names)[np.argsort(np.array(country_names))]\n",
    "for abv in sorted_countries[bine_labels == 0]:\n",
    "    print(country_abv_to_full[abv])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relabeled_bine_labels = np.zeros(bine_labels.shape)\n",
    "\n",
    "i = 0\n",
    "for label in bine_labels:\n",
    "    if label ==1 :\n",
    "        relabeled_bine_labels[i] = 0\n",
    "    elif label == 0:\n",
    "        relabeled_bine_labels[i] = 1\n",
    "    else:\n",
    "        relabeled_bine_labels[i] =2 \n",
    "\n",
    "    i+=1\n",
    "\n",
    "for abv in sorted_countries[relabeled_bine_labels == 0]:\n",
    "    print(country_abv_to_full[abv])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Largest Change in Countries\n",
    "\n",
    "Now that we have completed change point detection, we want to be able to analysis what caused the large change between years. For this we need to analysis how much a country changed between years. We can easily accomplish this by calculating the similarity between policies for countries in a range of years and find which country had the largest change between years. We don't use the bine embeddings of countries here because we don't have bine embeddings for every country. A country that is not implementing a policy will not have an embedding which causes challenges when calculating the change over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yearly_adjacency_dict = {}\n",
    "sorted_greenbond_countries = np.sort(green_bond_country_nodes)\n",
    "\n",
    "for year in green_bond_years:\n",
    "    columns = []\n",
    "    B.remove_edges_from(B.edges)\n",
    "    B.add_weighted_edges_from(green_bond_dict[year])\n",
    "    countries_in_green_bonds = []\n",
    "\n",
    "    for node in B.nodes(data=True):\n",
    "\n",
    "        # obtains nodes in partition 0, which is the country nodes\n",
    "        if node[1][\"bipartite\"] == 0:\n",
    "\n",
    "            col = np.zeros((2,1))\n",
    "            # Obtains list of all edges that country is in\n",
    "            for connect in B.edges(node[0], data = True):\n",
    "                # Gets weight for Green Bonds Issuances\n",
    "                if connect[1] == \"Green Bonds Issuances\":\n",
    "                    col[0,0] = connect[2][\"weight\"]\n",
    "                # Gets weight for NonGreen Bonds Issuances\n",
    "                else:\n",
    "                    col[1,0] = connect[2][\"weight\"]\n",
    "\n",
    "            countries_in_green_bonds.append(node[0])\n",
    "            columns.append(col)\n",
    "\n",
    "    sorted_indicies = np.argsort(np.array(countries_in_green_bonds))\n",
    "    graph_data = np.concatenate(columns, axis = 1)[:,sorted_indicies]\n",
    "    \n",
    "\n",
    "    yearly_adjacency_dict[year] = graph_data\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a dataframe that stores all of the matrices, we can now compute the differences between years with a given window size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_dicts = {}\n",
    "\n",
    "def country_difference(windowSize, adjacency_dict, years, countries):\n",
    "    \n",
    "    window_dicts = {}\n",
    "    for i in range(len(years) - windowSize):\n",
    "        year_subset = years[i:i+windowSize]\n",
    "\n",
    "        # Choose one random signature to start our weights_avg\n",
    "        weights_avg = np.zeros(adjacency_dict[years[0]].shape)\n",
    "\n",
    "        for year in year_subset:\n",
    "            weights_avg += adjacency_dict[year] * (1/windowSize)\n",
    "\n",
    "        window_dicts[years[windowSize + i]] = weights_avg / np.linalg.norm(weights_avg, ord = 2)\n",
    "\n",
    "\n",
    "    country_change = {}\n",
    "\n",
    "    for year in window_dicts.keys():\n",
    "        baseline = window_dicts[year]\n",
    "        single_year = adjacency_dict[year]\n",
    "        \n",
    "        change = np.power(baseline - single_year, 2).sum(axis = 0)\n",
    "        sort_indices = np.argsort(change)[::-1]\n",
    "        info = (countries[sort_indices], change[sort_indices])\n",
    "\n",
    "        country_change[year] = info\n",
    "\n",
    "    return country_change\n",
    "\n",
    "\n",
    "country_change = country_difference(6,yearly_adjacency_dict, green_bond_years, sorted_greenbond_countries)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2014\n",
    "\n",
    "countryOrder_abv, changeScore = country_change[year]\n",
    "\n",
    "countryOrder = [country_abv_to_full[country] for country in countryOrder_abv[changeScore > 0]]\n",
    "changeScore = changeScore[changeScore > 0]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.barh(countryOrder[:15], changeScore[:15])\n",
    "ax.set_title(f\"Most Change in Policies in {year}\")\n",
    "ax.set_ylabel(\"Countries\")\n",
    "ax.set_xlabel(\"Change (Euclidean Distance)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countryOrder_abv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "green_bonds[green_bonds[\"ISO2\"] == \"CN\"][[\"ISO2\",\"Indicator\",\"F2012\",\"F2013\", \"F2014\", \"F2015\", \"F2016\", \"F2017\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "green_bonds[green_bonds[\"ISO2\"] == \"CN\"][[\"ISO2\",\"Indicator\",\"F2012\",\"F2013\", \"F2014\", \"F2015\", \"F2016\", \"F2017\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "green_bonds[green_bonds[\"ISO2\"] == \"US\"][[\"ISO2\",\"Indicator\",\"F2012\",\"F2013\", \"F2014\", \"F2015\"]]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environmental Taxes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First lets load in the data and take a quick peak.  The final table is a table about how many measures each country implements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxes = pd.read_csv(\"input/policy/Environmental_Taxes.csv\")\n",
    "\n",
    "num_countries = len(taxes[\"Country\"].unique())\n",
    "print(f\"There are {num_countries} unique countries that are implementing environmental taxes\")\n",
    "print(\"Each country typically reports 10 different measures, but there are a few that report less than 10 measures\")\n",
    "\n",
    "taxes.Country.value_counts().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tax_country_nodes = taxes.ISO2.unique()\n",
    "tax_policy_nodes = taxes.CTS_Code.unique()\n",
    "print(\"Number of Countries:\", len(country_nodes))\n",
    "print(\"Number of Subsidies:\", len(policy_nodes))\n",
    "\n",
    "T = nx.Graph()\n",
    "T.add_nodes_from(country_nodes, bipartite=0)\n",
    "T.add_nodes_from(policy_nodes, bipartite=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tax_edge_dict = {}\n",
    "\n",
    "taxes_sub = taxes[taxes.Unit == \"Percent of GDP\"]\n",
    "for year in taxes_sub.columns[10:]:\n",
    "    edge_list = []\n",
    "    for country in tax_country_nodes:\n",
    "        subset = taxes_sub[taxes_sub.ISO2 == country]\n",
    "        for policy in tax_policy_nodes:\n",
    "            weight = subset[subset.CTS_Code == policy][year].values\n",
    "            if weight > 0:\n",
    "                edge = (country, policy, weight[0])\n",
    "                edge_list.append(edge)\n",
    "    tax_edge_dict[int(year[1:])] = edge_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T.add_weighted_edges_from(edges)\n",
    "print(\"Number of Edges:\", T.number_of_edges())\n",
    "print(\"Number of Possible Edges:\", len(country_nodes) * len(policy_nodes))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxes_no_zeros = taxes_sub.fillna(0)\n",
    "graph_years = []\n",
    "graph_count = []\n",
    "graph_country_count = []\n",
    "\n",
    "for year in taxes_sub.columns[10:]:\n",
    "    implement_count = (taxes_no_zeros[year] > 0).sum()\n",
    "\n",
    "    b = taxes_no_zeros[year] > 0\n",
    "    countries = taxes[taxes[\"Unit\"] == \"Percent of GDP\"][\"ISO2\"]\n",
    "    num_countries = len(countries[b].unique())\n",
    "\n",
    "    graph_years.append(int(year[1:]))\n",
    "    graph_count.append(implement_count)\n",
    "    graph_country_count.append(num_countries)\n",
    "\n",
    "# Plot\n",
    "fig,ax = plt.subplots()\n",
    "# Using years var from last cell\n",
    "ax.plot(graph_years, graph_count)\n",
    "ax.set_title(\"Number of Tax Policies being Implemented\")\n",
    "ax.set_ylabel(\"Number of Policies Implemented\")\n",
    "ax.set_xlabel(\"Year\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Plot\n",
    "fig,ax = plt.subplots()\n",
    "# Using years var from last cell\n",
    "ax.plot(graph_years, graph_country_count)\n",
    "ax.set_title(\"Number of Countries Implementing Tax Policies\")\n",
    "ax.set_ylabel(\"Number of Countries implementing\")\n",
    "ax.set_xlabel(\"Year\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another simple analysis I want to have is how many countries are implementing tax policies each year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots()\n",
    "# Using years var from last cell\n",
    "ax.plot(year_list, Z_list)\n",
    "ax.set_title(\"Change Point Detection for Green Bonds \\n Window of 6 years\")\n",
    "ax.set_ylabel(\"Change Value\")\n",
    "ax.set_xlabel(\"Year\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change Point "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tax_years = []\n",
    "for key in tax_edge_dict.keys():\n",
    "    tax_years.append(key)\n",
    "\n",
    "windowSize = 2\n",
    "Z_list, year_list = change_point_detection(windowSize, T, tax_years, tax_edge_dict)\n",
    "\n",
    "fig,ax = plt.subplots()\n",
    "# Using years var from last cell\n",
    "ax.plot(year_list, Z_list)\n",
    "ax.set_title(\"Change Point Detection Taxes\")\n",
    "ax.set_ylabel(\"Change Value\")\n",
    "ax.set_xlabel(\"Year\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxes_node_mapping = {}\n",
    "\n",
    "i = 0\n",
    "for country in taxes[\"ISO2\"].unique():\n",
    "    taxes_node_mapping[country] = f\"u{i}\"\n",
    "    i += 1\n",
    "\n",
    "\n",
    "j = 0\n",
    "for policy in taxes[\"CTS_Code\"].unique():\n",
    "    taxes_node_mapping[policy] = f\"i{j}\"\n",
    "    j += 1\n",
    "\n",
    "inv_taxes_mapping = {v: k for k, v in taxes_node_mapping.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create files \n",
    "for year in tax_years:\n",
    "    T.remove_edges_from(T.edges)\n",
    "    T.add_weighted_edges_from(tax_edge_dict[year])\n",
    "\n",
    "    T_integer = nx.relabel_nodes(T, taxes_node_mapping)\n",
    "\n",
    "    fileName = f\"BiNE-master/data/taxes/taxes_{year}_edgelist.dat\"\n",
    "    nx.write_weighted_edgelist(T_integer, fileName, delimiter = \"\\t\")\n",
    "\n",
    "with open(\"Taxes_commands.txt\", \"w\") as f:\n",
    "    for year in tax_years:\n",
    "        command = f\"python train.py --train-data ../data/taxes/taxes_{year}_edgelist.dat --lam 0.025 --max-iter 100 --large 2 --vectors-u ../data/taxes/vectors_u_{year}.dat --vectors-v ../data/taxes/vectors_v_{year}.dat &&\"\n",
    "        f.write(command)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environmental Protection Expenditures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expenditures = pd.read_csv(\"input/policy/Environmental_Protection_Expenditures.csv\", index_col=0, keep_default_na=False, na_values=\"\")\n",
    "expenditures.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_country_nodes = expenditures.ISO2.unique()\n",
    "exp_policy_nodes = expenditures.CTS_Code.unique()\n",
    "print(\"Number of countries:\", len(country_nodes))\n",
    "print(\"Number of expenditures:\", len(policy_nodes))\n",
    "\n",
    "E = nx.Graph()\n",
    "E.add_nodes_from(country_nodes, bipartite=0)\n",
    "E.add_nodes_from(policy_nodes, bipartite=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expenditure_edge_dict = {}\n",
    "\n",
    "exp_sub = expenditures[expenditures.Unit == \"Percent of GDP\"]\n",
    "for year in exp_sub.columns[9:]:\n",
    "    edge_list = []\n",
    "    for country in exp_country_nodes:\n",
    "        subset = exp_sub[exp_sub.ISO2 == country]\n",
    "        for policy in exp_policy_nodes:\n",
    "            weight = subset[subset.CTS_Code == policy][year].values\n",
    "            if weight.size > 0 and weight > 0:\n",
    "                edge = (country, policy, weight[0])\n",
    "                edge_list.append(edge)\n",
    "    expenditure_edge_dict[int(year[1:])] = edge_list\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of possible edges:\", len(country_nodes) * len(policy_nodes))\n",
    "print(\"Number of edges:\", len(edges))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change Point Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expenditures_years = []\n",
    "for key in expenditure_edge_dict.keys():\n",
    "    expenditures_years.append(key)\n",
    "\n",
    "windowSize = 3\n",
    "Z_list, year_list = change_point_detection(windowSize, E, expenditures_years, expenditure_edge_dict)\n",
    "\n",
    "\n",
    "fig,ax = plt.subplots()\n",
    "# Using years var from last cell\n",
    "ax.plot(year_list, Z_list)\n",
    "ax.set_title(\"Change Point Detection Expenditures\")\n",
    "ax.set_ylabel(\"Change Value\")\n",
    "ax.set_xlabel(\"Year\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for comp in nx.connected_components(B):\n",
    "    print(comp)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expenditures_node_mapping = {}\n",
    "\n",
    "i = 0\n",
    "for country in expenditures[\"ISO2\"].unique():\n",
    "    expenditures_node_mapping[country] = f\"u{i}\"\n",
    "    i += 1\n",
    "\n",
    "\n",
    "j = 0\n",
    "for policy in expenditures[\"CTS_Code\"].unique():\n",
    "    expenditures_node_mapping[policy] = f\"i{j}\"\n",
    "    j += 1\n",
    "\n",
    "inv_taxes_mapping = {v: k for k, v in expenditures_node_mapping.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create files \n",
    "for year in expenditures_years:\n",
    "    E.remove_edges_from(E.edges)\n",
    "    E.add_weighted_edges_from(expenditure_edge_dict[year])\n",
    "\n",
    "    E_integer = nx.relabel_nodes(E, expenditures_node_mapping)\n",
    "\n",
    "    fileName = f\"BiNE-master/data/expenditures/expenditures_{year}_edgelist.dat\"\n",
    "    nx.write_weighted_edgelist(E_integer, fileName, delimiter = \"\\t\")\n",
    "\n",
    "# Writes command line prompt to file\n",
    "with open(\"Expenditures_commands.txt\", \"w\") as f:\n",
    "    for year in tax_years:\n",
    "        command = f\"python train.py --train-data ../data/expenditures/expenditures_{year}_edgelist.dat --lam 0.025 --max-iter 100 --large 2 --vectors-u ../data/expenditures/vectors_u_{year}.dat --vectors-v ../data/expenditures/vectors_v_{year}.dat &&\"\n",
    "        f.write(command)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
